---
title: Observer Definitions First - P2P is KEy
description: In order to understand the world, it is absolutely recommended to define who is receiver, observer - the human brain who has the gap and how the information needs to be to fill that
date: 2023-02-12
tags:
  - Individual 
  - Information
  - Entropy
  - P2P
  - Wolfram
  - Physics
  - Data Mining
---

## Process of Measurements in Physics or any Science - Individual Information Gathering & Transfer : Observer vs World

All human start life with very specific individual but highly constraint condition - and begin all learning process by starting be observers first.

What is entropy doing here? There is near infinite information outside this new individuum and the gap of understanding the world outside is max. If our human brain would be of the size of the entire world, we would ´know´everything immidiatly, but would be exactly same thing as the entire world - had to do nothing, nothing to gain - all would be fine. Equilibriums are the death of any new thing, any process that generates new things and information. Information btw is a two folded thing cause the gradient needs to be there > 0 or there is no information. It is a process of distinct peers and a gap between.

Very great to understand here with Seven Wolfram about the Second Law of Thermodynamics

https://youtu.be/PdE-waSx-d8?t=11757

and try to compare to derive basics of 3 big fields in physics
Thermodynamics TD, Quantum Mechanics QM and Space/Time Relativity Theory RT

and all need distinct & defined = reduced Observers

https://youtu.be/PdE-waSx-d8?t=13971

for us human to get a chance to gather understandable information P2P like

In QM we already understand such a P2P and observer concept - we have to integrate over 2-fold probability (complex information functions - Schroedinger) functions to ´realize´ things from any ´remote´experimental event - only that way we can observe and learn sth from very small - the smallests - things but Heisinger Uncertainty is already the limit here.

Same folding of information functions needs to be considered everywhere - in TD: where multi particle events (unbounded)  needs to get averaged - Statistical Physics - to let us understand things - limited/bounded.

Complicated / complex things must always get casted down on very limited levels / models to let us learn the gap, the gradient or increment of information and that can be extended to anything of human process things. > Computers Coding - the new way we try to get information sorted and do new things like process mining, Bitcoin/Blockchain mining or even code level mining - all data or information mining.


 


What else?
